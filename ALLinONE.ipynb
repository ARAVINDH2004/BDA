{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Sg64jx10XwI3",
        "outputId": "6466ac93-37d3-4f9d-e424-70cdd42e1ccd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[[0.46824062 0.5564131  0.46830392 ... 0.49306852 0.4061299  0.46411484]\n",
            "  [0.47598845 0.55510455 0.47800186 ... 0.49304226 0.40060428 0.4702602 ]\n",
            "  [0.47095585 0.5524668  0.47349644 ... 0.49247035 0.41551554 0.4656694 ]\n",
            "  ...\n",
            "  [0.473594   0.5530096  0.48761848 ... 0.490785   0.40739113 0.47809464]\n",
            "  [0.47385246 0.5515101  0.47469315 ... 0.48799852 0.40717867 0.47817177]\n",
            "  [0.45735514 0.5545202  0.47318476 ... 0.48771787 0.41125482 0.45760006]]\n",
            "\n",
            " [[0.5257553  0.46964663 0.64925057 ... 0.5456276  0.62523377 0.49289626]\n",
            "  [0.51869744 0.48040384 0.64571327 ... 0.53287935 0.6220018  0.50445914]\n",
            "  [0.53110534 0.48170853 0.6410709  ... 0.5455302  0.6282068  0.4921141 ]\n",
            "  ...\n",
            "  [0.53149515 0.48453844 0.63667035 ... 0.52498525 0.6216751  0.5099271 ]\n",
            "  [0.52042365 0.48426446 0.64478606 ... 0.5371342  0.6203686  0.5011124 ]\n",
            "  [0.51877236 0.48158753 0.6400035  ... 0.5273335  0.62763    0.5056677 ]]\n",
            "\n",
            " [[0.5787959  0.49524885 0.5873417  ... 0.59600276 0.670287   0.5746271 ]\n",
            "  [0.5716399  0.46866807 0.5732269  ... 0.5869684  0.6659053  0.5821252 ]\n",
            "  [0.5816656  0.47137818 0.5766403  ... 0.5954376  0.6703508  0.5826051 ]\n",
            "  ...\n",
            "  [0.56678414 0.46349686 0.57416356 ... 0.58517593 0.66885966 0.58109426]\n",
            "  [0.5625992  0.4728158  0.579663   ... 0.57180244 0.68265206 0.5799392 ]\n",
            "  [0.5626923  0.46991944 0.57713014 ... 0.5713144  0.6788989  0.58153445]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[0.5918595  0.5037146  0.48920798 ... 0.4991247  0.5193292  0.4425725 ]\n",
            "  [0.5944681  0.5018796  0.4839076  ... 0.507101   0.52031857 0.4513293 ]\n",
            "  [0.5994351  0.5003228  0.49047107 ... 0.5067741  0.51234823 0.4497116 ]\n",
            "  ...\n",
            "  [0.5988864  0.51105756 0.47838593 ... 0.5241687  0.50811034 0.4443511 ]\n",
            "  [0.5946249  0.49314207 0.48902366 ... 0.5243883  0.50715876 0.44299513]\n",
            "  [0.607878   0.49626657 0.48526415 ... 0.52623224 0.5060221  0.44728184]]\n",
            "\n",
            " [[0.53501385 0.5384202  0.6192722  ... 0.4600581  0.49126065 0.46791285]\n",
            "  [0.5399567  0.52438074 0.6234798  ... 0.45036888 0.4898869  0.46677792]\n",
            "  [0.5382602  0.5239291  0.62087303 ... 0.44901237 0.48433787 0.46670645]\n",
            "  ...\n",
            "  [0.53283656 0.5314714  0.62035793 ... 0.44892335 0.48647508 0.47543442]\n",
            "  [0.5463698  0.51945055 0.61222196 ... 0.44945568 0.47570437 0.47964764]\n",
            "  [0.54493695 0.5243834  0.61260504 ... 0.45642948 0.4902649  0.4641792 ]]\n",
            "\n",
            " [[0.4897257  0.61490774 0.48279065 ... 0.5404262  0.47191083 0.44551885]\n",
            "  [0.4990546  0.6027929  0.4930894  ... 0.5288796  0.469956   0.45995677]\n",
            "  [0.47893417 0.6204767  0.47117668 ... 0.54720545 0.47948924 0.4494522 ]\n",
            "  ...\n",
            "  [0.49767098 0.62069374 0.46130455 ... 0.5316315  0.4963628  0.4426992 ]\n",
            "  [0.49511722 0.611762   0.4875962  ... 0.5342972  0.46429303 0.45405385]\n",
            "  [0.4907725  0.6169314  0.4653545  ... 0.53038204 0.48037362 0.4596329 ]]], shape=(32, 10, 64), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "#SCALED DOT PRODUCT\n",
        "from IPython import get_ipython\n",
        "from IPython.display import display\n",
        "from tensorflow import matmul, cast, float32, math\n",
        "from tensorflow.math import sqrt\n",
        "from tensorflow.keras.layers import Layer\n",
        "from tensorflow.keras.activations import softmax\n",
        "import numpy as np\n",
        "\n",
        "class DotProductAttention(Layer):\n",
        "  def __init__(self, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "\n",
        "  def call(self, queries, keys, values, *, d_k, mask=None):\n",
        "    scores = matmul(queries, keys, transpose_b=True) / sqrt(cast(d_k, float32))\n",
        "    if mask is not None:\n",
        "      scores += -1e9 * mask\n",
        "    weights = softmax(scores)\n",
        "    return matmul(weights, values)\n",
        "\n",
        "batch_size = 32\n",
        "input_seq_length = 10\n",
        "d_k = 64\n",
        "d_v = 64\n",
        "\n",
        "random = np.random.default_rng(seed=42)\n",
        "queries = random.random((batch_size, input_seq_length, d_k))\n",
        "keys = random.random((batch_size, input_seq_length, d_k))\n",
        "values = random.random((batch_size, input_seq_length, d_v))\n",
        "attention = DotProductAttention()\n",
        "print(attention(queries, keys, values, d_k=d_k))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython import get_ipython\n",
        "from IPython.display import display\n",
        "# %%\n",
        "from tensorflow import math, matmul, reshape, shape, transpose, cast, float32, concat\n",
        "from tensorflow.keras.layers import Dense, Layer\n",
        "from tensorflow.keras.backend import softmax\n",
        "# Implementing the Scaled-Dot Product Attention\n",
        "\n",
        "class DotProductAttention(Layer):\n",
        "  def __init__(self, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "\n",
        "  def call(self, queries, keys, values, mask=None):\n",
        "    d_k = queries.shape[-1]\n",
        "    # Scoring the queries against the keys after transposing the latter, and scaling\n",
        "    scores = matmul(queries, keys, transpose_b=True) / math.sqrt(cast(d_k, float32))\n",
        "    # Apply mask to the attention scores\n",
        "    if mask is not None:\n",
        "      scores += -1e9 * mask\n",
        "    # Computing the weights by a softmax operation\n",
        "    weights = softmax(scores)\n",
        "    # Computing the attention by a weighted sum of the value vectors\n",
        "    return matmul(weights, values)\n",
        "    # Implementing the Multi-Head Attention\n",
        "\n",
        "class MultiHeadAttention(Layer):\n",
        "  def __init__(self, h, d_k, d_v, d_model, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.attention = DotProductAttention() # Scaled dot product attention\n",
        "    self.heads = h # Number of attention heads to use\n",
        "    self.d_k = d_k # Dimensionality of the linearly projected queries and keys\n",
        "    self.d_v = d_v # Dimensionality of the linearly projected values\n",
        "    self.d_model = d_model # Dimensionality of the model\n",
        "    self.W_q = Dense(d_k) # Learned projection matrix for the queries\n",
        "    self.W_k = Dense(d_k) # Learned projection matrix for the keys\n",
        "    self.W_v = Dense(d_v) # Learned projection matrix for the values\n",
        "    self.W_o = Dense(d_model) # Learned projection matrix for the multi-head output\n",
        "\n",
        "  def reshape_tensor(self, x, heads, flag):\n",
        "    if flag:\n",
        "      # Tensor shape after reshaping and transposing:\n",
        "      # (batch_size, heads, seq_length, -1)\n",
        "      x = reshape(x, shape=(shape(x)[0], shape(x)[1], heads, -1))\n",
        "      x = transpose(x, perm=(0, 2, 1, 3))\n",
        "    else:\n",
        "        x = transpose(x, perm=(0, 2, 1, 3))\n",
        "        x_shape = shape(x)\n",
        "        new_shape = (x_shape[0], x_shape[1], x_shape[2] * x_shape[3])\n",
        "        x = reshape(x, new_shape)\n",
        "\n",
        "    return x\n",
        "\n",
        "  def call(self, queries, keys, values, mask=None):\n",
        "    # Rearrange the queries to be able to compute all heads in parallel\n",
        "    q_reshaped = self.reshape_tensor(self.W_q(queries), self.heads, True)\n",
        "    # Resulting tensor shape: (batch_size, heads, input_seq_length, -1)\n",
        "    # Rearrange the keys to be able to compute all heads in parallel\n",
        "    k_reshaped = self.reshape_tensor(self.W_k(keys), self.heads, True)\n",
        "    # Resulting tensor shape: (batch_size, heads, input_seq_length, -1)\n",
        "    # Rearrange the values to be able to compute all heads in parallel\n",
        "    v_reshaped = self.reshape_tensor(self.W_v(values), self.heads, True)\n",
        "    # Resulting tensor shape: (batch_size, heads, input_seq_length, -1)\n",
        "    # Compute the multi-head attention output using the reshaped queries,\n",
        "    # keys, and values\n",
        "    o_reshaped = self.attention(q_reshaped, k_reshaped, v_reshaped, mask=mask)\n",
        "    # Resulting tensor shape: (batch_size, heads, input_seq_length, -1)\n",
        "    # Rearrange back the output into concatenated form\n",
        "    output = self.reshape_tensor(o_reshaped, self.heads, False)\n",
        "    # Resulting tensor shape: (batch_size, input_seq_length, d_model)\n",
        "    return self.W_o(output)\n",
        "# %%\n",
        "from numpy import random\n",
        "input_seq_length = 5 # Maximum length of the input sequence\n",
        "h = 8 # Number of self-attention heads\n",
        "d_k = 64 # Dimensionality of the linearly projected queries and keys\n",
        "d_v = 64 # Dimensionality of the linearly projected values\n",
        "d_model = 512 # Dimensionality of the model sub-layers' outputs\n",
        "batch_size = 64 # Batch size from the training process\n",
        "queries = random.random((batch_size, input_seq_length, d_k))\n",
        "keys = random.random((batch_size, input_seq_length, d_k))\n",
        "values = random.random((batch_size, input_seq_length, d_v))\n",
        "multihead_attention = MultiHeadAttention(h, d_k, d_v, d_model)\n",
        "print(multihead_attention(queries, keys, values))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "mGph6WV6X-mP",
        "outputId": "5d35314e-fd2f-492e-f30f-7d201c67d151"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[[-0.03918857  0.23965558  0.1576894  ... -0.09452695 -0.00175945\n",
            "   -0.01606564]\n",
            "  [-0.04583649  0.23292817  0.15202665 ... -0.09762093 -0.01820333\n",
            "   -0.00944021]\n",
            "  [-0.04153083  0.24064599  0.15618199 ... -0.09684861 -0.01875615\n",
            "   -0.0104829 ]\n",
            "  [-0.03141589  0.23563994  0.15689145 ... -0.08968071 -0.00576321\n",
            "   -0.01493201]\n",
            "  [-0.03767595  0.23066333  0.15056081 ... -0.09380836 -0.0031466\n",
            "   -0.01511911]]\n",
            "\n",
            " [[-0.154156    0.297374    0.18527241 ... -0.16654359 -0.07911475\n",
            "    0.09892107]\n",
            "  [-0.15713435  0.2952929   0.18469529 ... -0.1649993  -0.07346685\n",
            "    0.0963065 ]\n",
            "  [-0.14870173  0.29774722  0.18671434 ... -0.17230955 -0.06972022\n",
            "    0.10104506]\n",
            "  [-0.14972018  0.28962058  0.17733829 ... -0.16820109 -0.07686087\n",
            "    0.09654674]\n",
            "  [-0.14786234  0.29783407  0.18618785 ... -0.15618616 -0.07385553\n",
            "    0.09621389]]\n",
            "\n",
            " [[-0.11995585  0.25221106  0.25074473 ... -0.05550449 -0.07291637\n",
            "    0.13295746]\n",
            "  [-0.1236603   0.2379325   0.25063935 ... -0.06035953 -0.07826111\n",
            "    0.13054794]\n",
            "  [-0.12296809  0.23998937  0.2537986  ... -0.06689239 -0.07089054\n",
            "    0.12617551]\n",
            "  [-0.11408666  0.25563884  0.24401091 ... -0.05437205 -0.07610673\n",
            "    0.14160843]\n",
            "  [-0.11485706  0.24974278  0.24185519 ... -0.05920061 -0.08674864\n",
            "    0.1398278 ]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-0.20858745  0.12820303  0.21731147 ...  0.03748905 -0.12614879\n",
            "    0.13735415]\n",
            "  [-0.2118644   0.12861817  0.21696603 ...  0.03167762 -0.11176893\n",
            "    0.14074303]\n",
            "  [-0.21010922  0.12530483  0.2204109  ...  0.03788394 -0.11788994\n",
            "    0.13758895]\n",
            "  [-0.20951945  0.13000712  0.22056684 ...  0.04016702 -0.11897971\n",
            "    0.13886496]\n",
            "  [-0.20856336  0.12581946  0.21694396 ...  0.03163493 -0.1177558\n",
            "    0.14409588]]\n",
            "\n",
            " [[-0.24872622  0.22893418  0.2545189  ... -0.08692288 -0.1743485\n",
            "    0.20646702]\n",
            "  [-0.24506088  0.2272223   0.253257   ... -0.08796164 -0.18708874\n",
            "    0.21229482]\n",
            "  [-0.25630403  0.22720732  0.24878663 ... -0.08804928 -0.17608522\n",
            "    0.20407793]\n",
            "  [-0.25916758  0.2375788   0.250655   ... -0.08044439 -0.16750517\n",
            "    0.20704174]\n",
            "  [-0.2503053   0.23546126  0.25798333 ... -0.08620623 -0.17866962\n",
            "    0.20637366]]\n",
            "\n",
            " [[-0.11765324  0.20158626  0.21067123 ...  0.01692033 -0.02415414\n",
            "    0.2296055 ]\n",
            "  [-0.11198083  0.19735648  0.20625111 ...  0.0192475  -0.01591854\n",
            "    0.2208436 ]\n",
            "  [-0.10838962  0.20268002  0.19338499 ...  0.0152997  -0.0290333\n",
            "    0.24610536]\n",
            "  [-0.11210378  0.2006836   0.20171963 ...  0.01138784 -0.02713474\n",
            "    0.23156062]\n",
            "  [-0.11826225  0.20516011  0.20419343 ...  0.02284451 -0.03585465\n",
            "    0.24135439]]], shape=(64, 5, 512), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, units):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        self.W1 = tf.keras.layers.Dense(units)  # Dense layer for the encoder hidden states\n",
        "        self.W2 = tf.keras.layers.Dense(units)  # Dense layer for the decoder hidden state\n",
        "        self.V = tf.keras.layers.Dense(1)       # Dense layer to compute alignment scores\n",
        "\n",
        "    def call(self, query, values):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            query: Decoder hidden state (shape: [batch_size, hidden_size]).\n",
        "            values: Encoder outputs (shape: [batch_size, seq_len, hidden_size]).\n",
        "        Returns:\n",
        "            context_vector: Weighted sum of encoder outputs (shape: [batch_size, hidden_size]).\n",
        "            attention_weights: Attention weights (shape: [batch_size, seq_len]).\n",
        "        \"\"\"\n",
        "        # Add time axis to query for broadcasting (shape: [batch_size, 1, hidden_size])\n",
        "        query_with_time_axis = tf.expand_dims(query, 1)\n",
        "\n",
        "        # Compute the alignment scores (shape: [batch_size, seq_len, 1])\n",
        "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(query_with_time_axis)))\n",
        "\n",
        "        # Remove the last axis (shape: [batch_size, seq_len])\n",
        "        attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "        # Compute the context vector as the weighted sum of values (shape: [batch_size, hidden_size])\n",
        "        context_vector = tf.reduce_sum(attention_weights * values, axis=1)\n",
        "\n",
        "        return context_vector, attention_weights\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Define batch size, sequence length, and hidden size\n",
        "    batch_size = 64\n",
        "    seq_len = 10\n",
        "    hidden_size = 256\n",
        "    attention_units = 128\n",
        "\n",
        "    # Instantiate the attention layer\n",
        "    attention = BahdanauAttention(units=attention_units)\n",
        "\n",
        "    # Simulated encoder outputs (values) and decoder hidden state (query)\n",
        "    encoder_outputs = tf.random.normal([batch_size, seq_len, hidden_size])\n",
        "    decoder_hidden_state = tf.random.normal([batch_size, hidden_size])\n",
        "\n",
        "    # Apply the attention mechanism\n",
        "    context_vector, attention_weights = attention(decoder_hidden_state, encoder_outputs)\n",
        "\n",
        "    print(\"Context vector shape:\", context_vector.shape)  # Expected: [batch_size, hidden_size]\n",
        "    print(\"Attention weights shape:\", attention_weights.shape)  # Expected: [batch_size, seq_len]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "4PxLmp2TYCnj",
        "outputId": "e3965fae-3e95-4140-d02e-674e1f8f37a9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context vector shape: (64, 256)\n",
            "Attention weights shape: (64, 10, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Layer, Dense\n",
        "\n",
        "class LuongAttention(Layer):\n",
        "    def __init__(self, attention_type, hidden_size):\n",
        "        super(LuongAttention, self).__init__()\n",
        "        self.attention_type = attention_type\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        if attention_type == \"general\":\n",
        "            self.attention_weight = Dense(hidden_size)\n",
        "        elif attention_type == \"concat\":\n",
        "            self.attention_weight = Dense(hidden_size)\n",
        "            self.v = tf.Variable(tf.random.normal([hidden_size]), trainable=True)\n",
        "\n",
        "    def score(self, hidden, encoder_outputs):\n",
        "        if self.attention_type == \"dot\":\n",
        "            # Dot product between hidden state and encoder outputs\n",
        "            return tf.matmul(encoder_outputs, tf.expand_dims(hidden, axis=-1))[:, :, 0]\n",
        "\n",
        "        elif self.attention_type == \"general\":\n",
        "            # Linear transformation followed by dot product\n",
        "            energy = self.attention_weight(encoder_outputs)\n",
        "            return tf.matmul(energy, tf.expand_dims(hidden, axis=-1))[:, :, 0]\n",
        "\n",
        "        elif self.attention_type == \"concat\":\n",
        "            # Concatenate hidden state with encoder outputs\n",
        "            hidden_expanded = tf.expand_dims(hidden, axis=1)\n",
        "            hidden_expanded = tf.tile(hidden_expanded, [1, tf.shape(encoder_outputs)[1], 1])\n",
        "            concat_input = tf.concat([hidden_expanded, encoder_outputs], axis=-1)\n",
        "            energy = tf.tanh(self.attention_weight(concat_input))\n",
        "            return tf.reduce_sum(energy * self.v, axis=2)\n",
        "\n",
        "        else:\n",
        "            raise ValueError(\"Unknown attention type: {}\".format(self.attention_type))\n",
        "\n",
        "    def call(self, hidden, encoder_outputs):\n",
        "        # Compute alignment scores\n",
        "        alignment_scores = self.score(hidden, encoder_outputs)\n",
        "\n",
        "        # Softmax normalization to obtain attention weights\n",
        "        attention_weights = tf.nn.softmax(alignment_scores, axis=1)\n",
        "\n",
        "        # Compute the context vector as the weighted sum of encoder outputs\n",
        "        context_vector = tf.matmul(tf.expand_dims(attention_weights, axis=1), encoder_outputs)\n",
        "        context_vector = tf.squeeze(context_vector, axis=1)\n",
        "\n",
        "        return context_vector, attention_weights\n",
        "\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    batch_size = 2\n",
        "    seq_len = 5\n",
        "    hidden_size = 10\n",
        "\n",
        "    # Simulated inputs\n",
        "    hidden = tf.random.normal([batch_size, hidden_size])  # Decoder hidden state\n",
        "    encoder_outputs = tf.random.normal([batch_size, seq_len, hidden_size])  # Encoder outputs\n",
        "\n",
        "    # Instantiate Luong Attention (dot, general, or concat)\n",
        "    attention_type = \"dot\"  # Options: \"dot\", \"general\", \"concat\"\n",
        "    attention_layer = LuongAttention(attention_type, hidden_size)\n",
        "\n",
        "    # Forward pass\n",
        "    context_vector, attention_weights = attention_layer(hidden, encoder_outputs)\n",
        "\n",
        "    print(\"Context vector:\", context_vector.numpy())\n",
        "    print(\"Attention weights:\", attention_weights.numpy())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "4JGYDm2yYEX9",
        "outputId": "2c631a66-477d-45c5-e293-0376d4456fbe"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context vector: [[-0.0061667  -0.4700908   0.26643655  0.3190401  -0.32473612 -0.93865937\n",
            "   0.49196798  0.90012634  0.26490387  0.10492379]\n",
            " [ 1.652067   -0.5170198   0.42716143  1.3397385   2.1753972   0.7432705\n",
            "   1.1176186   0.86316746  0.6020574   1.107633  ]]\n",
            "Attention weights: [[6.7430109e-02 1.8881023e-01 3.3759749e-01 3.9063013e-01 1.5531966e-02]\n",
            " [1.4719539e-04 1.3359219e-02 6.3651330e-03 8.2620652e-03 9.7186637e-01]]\n"
          ]
        }
      ]
    }
  ]
}